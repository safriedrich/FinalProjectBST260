---
title: "BST260 - Final project"
output: pdf_document
author: \textcolor{answercolor}{Sabine Friedrich}
date:  12/15/2022
header-includes: \definecolor{answercolor}{rgb}{0.27,0.51,0.71}
 \usepackage{fvextra}
  \DefineVerbatimEnvironment{Highlighting}{Verbatim}{breaklines,commandchars=\\\{\}}
---

# Introduction

The dataset that I will analyze was assembled by Korean investigators for a cross-sectional retrospective research study aiming to evaluate accuracy of triage in the emergency department by the Korean Triage and Acuity Scale. The original study report was published in 2019 (1) and the dataset was made available on kaggle.com.
This is a tidy dataset including 1267 records of adult patients who were admitted to the emergency department (ED) at two different hospitals between October 2016 and September 2017. It includes a variable detailing the disposition of each patient upon discharge from the ED. 
My initial plan to predict emergency surgery aiming to identify patients who may require emergency surgery early in order to reduce the time until start of the surgical procedure. However, there were only 22 patients (1.7%) who required emergency surgery upon exploratory analysis (Fig. 1). 
Accordingly, the aim of this project was adapted to predict inpatient admission (including mortality, or transfer to another hospital) in contrast to discharge home. The ability to predict hospital admission of ED patients may help guide and refine the triage process.

```{r 1 - Data exploration, appendix= TRUE, echo = FALSE, warning = FALSE, error = FALSE, message = FALSE, tidy=TRUE}
#read in data
library(readr)
emergency <- read_delim("~/Library/Mobile Documents/com~apple~CloudDocs/Fall2/BST260/emergency.csv", 
   delim = ";", escape_double = FALSE, trim_ws = TRUE)
  
#data exploration 
library(dplyr)
library(ggplot2)
# outcome
## Disposition: 1 = Discharge, 2 = Admission to ward, 3 = Admission to ICU, 4 = Discharge, 5 = Transfer, 6 = Death, 7 = Surgery
summary(as.factor(emergency$Disposition))
emergency$Disposition <- as.factor(ifelse(emergency$Disposition == 1 | emergency$Disposition == 4, 1, emergency$Disposition))
summary(emergency$Disposition)

#hist and provide histogram
label <- c("Discharge home", "Admission to ward", "Admission to ICU", "Transfer", "Died", "Emergency Surgery")
emergency$disposition <- factor(emergency$Disposition, levels = c(1, 2, 3, 5, 6, 7), labels = label)
emergency |> ggplot() + geom_bar(aes(disposition)) + xlab("Patient disposition") + ylab("Frequency") + ggtitle("Fig 1. Distribution of disposition location from ED") + scale_x_discrete(guide = guide_axis(n.dodge=2))
# binary outcome: discharge - disposition 1 of 4
emergency$admission <- ifelse(emergency$Disposition == 1 | emergency$Disposition == 4, 0, 1)
summary(as.factor(emergency$admission))
```

To identify predictors of inpatient admission, I will compare two approaches: 

- Clinical approach: pre-select candidate predictors based on clinical reasoning and expertise and then use an automated selection process to build and refine a regression model
- Machine learning approach: 


Model discrimination will be evaluated using overall accuracy, sensitivity, specificity and AUC. Model calibration will be assessed using a calibration plot comparing predicted probability and observed rates across deciles of predicted risk.


# Results
- 6+ key plots or tables illustrating your two major analyses 
guide the reader through your analysis and describe what each plot or table is showing, and how it relates to the central question you are trying to ask


```{r 2 - Data cleaning, appendix = TRUE, echo = FALSE, warning = FALSE, error = FALSE, message = FALSE}
##data cleaning

# use as is
## sex: 1 female, 2 male
## age: continuous in years
## mental: 1 = Alert, 2 = Verbal Response, 3 = Pain Response, 4 = Unresponsive
## chief complaint: text
## pain: yes=1, no = 0
## SBP: systolic blood pressure
## DBP: diastolic blood pressure
## HR: heart rate
## KTAS_RN: 1 = resuscitation, 2 = emergent, 3 = urgent, 4 = less urgent, 5 = non-urgent

# delete
## group, which ED - delete
## patients number per hour - unclear, also should not affect emergency surgery
## saturation - many missing, and available values range from 90 to 100, not very pathologic, no high predictive value to be expected
emergency <- emergency |> select(-Group, -`Patients number per hour`, -Saturation, -KTAS_expert, -Error_group, -mistriage, -`KTAS duration_min`)

#clean/rename
emergency$sbp <- as.numeric(emergency$SBP)
summary(emergency$sbp)
emergency$dbp <- as.numeric(emergency$DBP)
summary(emergency$dbp)
emergency$hr <- as.numeric(emergency$HR)
summary(emergency$hr)
emergency$resp <- as.numeric(emergency$RR)
summary(emergency$resp)
emergency$temp <- as.numeric(emergency$BT)
summary(emergency$temp)

emergency <- emergency |> select(-SBP, -DBP, -HR, -RR, -BT)

# recategorize
##arrival mode: 1 = Walking, 2 = Public Ambulance, 3 = Private Vehicle, 4 = Private Ambulance, 5,6,7 = Other]
## -> 1 = walking, 2 = ambulance, 3 = private vehicle, 4 = other
emergency$arrival <- ifelse(emergency$`Arrival mode`==2 | emergency$`Arrival mode`==4, 2, emergency$`Arrival mode`)
emergency$arrival <- ifelse(emergency$arrival==5 | emergency$arrival==6 | emergency$arrival==7, 4, emergency$arrival)
summary(as.factor(emergency$arrival))

## injury: 2=yes, 1=no -> 1 yes, 0 no
emergency$injury <- ifelse(emergency$Injury==2, 1, 0)
summary(as.factor(emergency$injury))

emergency <- emergency |> select(-Injury, -`Arrival mode`)


##NRS_pain: replace missing as 0, if they did not have pain
emergency$NRS_pain <-  as.numeric(emergency$NRS_pain)
emergency$NRS_pain <-  ifelse(is.na(emergency$NRS_pain) & emergency$Pain==0, 0, emergency$NRS_pain)
summary(as.factor(emergency$NRS_pain))

# generate additional out of existing predictors:
## shock index: HR/SBP
emergency$shock_index <- emergency$hr/emergency$sbp
summary(emergency$shock_index)
## shock: shock index > 1
emergency$shock <- ifelse(emergency$shock_index > 1, 1, 0) 
summary(as.factor(emergency$shock))

## hyperventilation: respiratory rate > 25
emergency$hyperventilation <- ifelse(emergency$resp > 25, 1, 0) 
summary(as.factor(emergency$shock))

#fix some column_names
emergency$ED_diagnosis <- emergency$`Diagnosis in ED`
emergency$ED_LOS_min <- emergency$`Length of stay_min`
emergency$chief_complaint <- emergency$Chief_complain
emergency$mental_status <- emergency$Mental
emergency$pain_yn <- emergency$Pain

## fever based on body temperature?
emergency$hypertherm <- ifelse(emergency$temp > 37.5 & !is.na(emergency$temp), 1, 0)
emergency <- emergency |> select(-`Diagnosis in ED`, -`Length of stay_min`, -Chief_complain, -Mental, -Pain)

## create a complete case cohort -> drop anyone with any missings as all variables kept in the dataset will be used for machine learning approach
ED_complete <- na.omit(emergency)
```
All cases with any missing values for potential predictors were removed and a total of 1228 cases remained in the complete case cohort. This cohort was split into a training dataset (80% of observations) and a validation set (remaining 20%).

```{r 4 - Data split, appendix = TRUE, echo = FALSE, warning = FALSE, error = FALSE, message = FALSE}
#creating a validation set with 20% of data
smp_size <- floor(0.80 * nrow(ED_complete))

## set the seed 
set.seed(2404)
train_ind <- sample(seq_len(nrow(ED_complete)), size = smp_size)

ED.train <- ED_complete[train_ind, ]
ED.test <- ED_complete[-train_ind, ]

#create table with inpatient admission rate for the whole dataset, training and validation model
# check outcome in separated sets
addmargins(table(as.factor(ED_complete$admission))); prop.table(table(as.factor(ED_complete$admission)))
# full dataset: patients with hearing difficulty: 1341/8798 (15.24%)
addmargins(table(as.factor(ED.train$admission))); prop.table(table(as.factor(ED.train$admission)))
# training sample: patients with hearing difficulty: 1007/6598 (15.26%)


```


```{r 5 - Clinician model, appendix = TRUE, echo = FALSE, warning = FALSE, error = FALSE, message = FALSE}
#training the clinician model: sex, age, NRS_pain, KTAS_RN, arrival, injury, shock, hyperventilation, mental status, hyperthermia/fever
library(caret)
#create a table displaying characteristics by outcome including the preselected variables

#stepwise forward regression with AIC as criterion
library(MASS)
Fitall.tr <- glm(admission ~ as.factor(Sex) + Age + NRS_pain + KTAS_RN + as.factor(arrival) + injury + shock + hyperventilation + mental_status + hypertherm, family="binomial", data= ED.train)

Fitstart <- glm(admission ~ 1, family="binomial", data= ED.train)

set.seed(2024)
m_clin <- stepAIC(Fitstart, scope=formula(Fitall.tr), direction="forward", k=2)
summary(m_clin)

#aaply to validation set
ED.train$phat_clin <- predict(m_clin, type="response", newdata=ED.train)
ED.test$phat_clin <- predict(m_clin, type="response", newdata=ED.test)

#Calibration Plot -training set
##create risk deciles on predicted risk
cuts <- quantile(ED.train$phat_clin, prob=c(.1,.2,.3,.4,.5,.6,.7,.8,.9), na.rm=T)
ED.train$risk_decile <-cut(ED.train$phat_clin, breaks=c(0, cuts, 1))
dec<-c(1:10) #for plot
#observed proportion of difficult hearing in risk deciles
t1.train<-table(ED.train$risk_decile, ED.train$admission)
addmargins(t1.train) 
t2.train <- prop.table(t1.train, 1)
obs.train <- t2.train[,2] #for plot
#mean predicted risk in risk deciles
deciles.train <- ED.train %>% group_by(risk_decile) %>% summarise(mean=mean(phat_clin))
pred.train <- deciles.train$mean #for plot
cali_train<-data.frame(dec, obs.train, pred.train) # for plot
ggplot(cali_train, aes(x=obs.train, y=pred.train)) + geom_point(size=2) + xlab("Observed Inpatient Admission Risk") + ylab("Predicted Risk") + ggtitle("Reliability Plot - Training Set") + theme(plot.title = element_text(hjust = 0.5)) + geom_abline(intercept = 0, slope = 1, color="grey")

#Calibration Plot -validation set
##create risk deciles on predicted risk
cuts <- quantile(ED.test$phat_clin, prob=c(.1,.2,.3,.4,.5,.6,.7,.8,.9), na.rm=T)
ED.test$risk_decile <-cut(ED.test$phat_clin, breaks=c(0, cuts, 1))
dec<-c(1:10) #for plot
#observed proportion of difficult hearing in risk deciles
t1.test<-table(ED.test$risk_decile, ED.test$admission)
addmargins(t1.test) 
t2.test <- prop.table(t1.test, 1)
obs.test <- t2.test[,2] #for plot
#mean predicted risk in risk deciles
deciles.test <- ED.test %>% group_by(risk_decile) %>% summarise(mean=mean(phat_clin))
pred.test <- deciles.test$mean #for plot
cali_test<-data.frame(dec, obs.test, pred.test) # for plot
ggplot(cali_test, aes(x=obs.test, y=pred.test)) + geom_point(size=2) + xlab("Observed Inpatient Admission Risk") + ylab("Predicted Risk") + ggtitle("Reliability Plot - Training Set") + theme(plot.title = element_text(hjust = 0.5)) + geom_abline(intercept = 0, slope = 1, color="grey")

#AUC to find good cutoff for predicting binary outcome based on predicted risk
library(pROC)
roccurve.train<- roc(ED.train$admission ~ ED.train$phat_clin); roccurve.train 
plot(roccurve.train, legacy.axes=T, main="ROC curve - Training Set", col="blue")

roccurve.test <- roc(ED.test$admission ~ ED.test$phat_clin); roccurve.train 
plot(roccurve.test, legacy.axes=T, main="ROC curve - Test Set", col="blue")


#check sensitivity and specificity for different cutoffs of predicted risk in training set
ED.train$admission <- as.factor(ED.train$admission)
## 0.3
ED.train$yhat_clin03 <- as.factor(ifelse(ED.train$phat_clin > 0.3, 1, 0))
cm_clinical_train03 <- confusionMatrix(ED.train$yhat_clin03, ED.train$admission, positive="1")
## 0.4
ED.train$yhat_clin04 <- as.factor(ifelse(ED.train$phat_clin > 0.4, 1, 0))
cm_clinical_train04 <- confusionMatrix(ED.train$yhat_clin04, ED.train$admission, positive="1")
## 0.5
ED.train$yhat_clin05 <- as.factor(ifelse(ED.train$phat_clin > 0.5, 1, 0))
cm_clinical_train05 <- confusionMatrix(ED.train$yhat_clin05, ED.train$admission, positive="1")
##0.6
ED.train$yhat_clin06 <- as.factor(ifelse(ED.train$phat_clin > 0.6, 1, 0))
cm_clinical_train06 <- confusionMatrix(ED.train$yhat_clin06, ED.train$admission, positive="1")
##0.7
ED.train$yhat_clin07 <- as.factor(ifelse(ED.train$phat_clin > 0.7, 1, 0))
cm_clinical_train07 <- confusionMatrix(ED.train$yhat_clin07, ED.train$admission, positive="1")
##0.8
ED.train$yhat_clin08 <- as.factor(ifelse(ED.train$phat_clin > 0.8, 1, 0))
cm_clinical_train08 <- confusionMatrix(ED.train$yhat_clin08, ED.train$admission, positive="1")

# best trade-off between sensitvity and secificity: cutoff: 0.3

#performance parameters for different cutoffs
rownames <- c("0.3","0.4","0.5", "0.6", "0.7", "0.8")
Specificity <- c(cm_clinical_train03$byClass["Specificity"],cm_clinical_train04$byClass["Specificity"],cm_clinical_train05$byClass["Specificity"],cm_clinical_train06$byClass["Specificity"],cm_clinical_train07$byClass["Specificity"],cm_clinical_train08$byClass["Specificity"])
Sensitivity <- c(cm_clinical_train03$byClass["Sensitivity"],cm_clinical_train04$byClass["Sensitivity"],cm_clinical_train05$byClass["Sensitivity"],cm_clinical_train06$byClass["Sensitivity"],cm_clinical_train07$byClass["Sensitivity"],cm_clinical_train08$byClass["Sensitivity"])
Accuracy <- c(cm_clinical_train03$overall["Accuracy"], cm_clinical_train04$overall["Accuracy"],cm_clinical_train05$overall["Accuracy"], cm_clinical_train06$overall["Accuracy"], cm_clinical_train07$overall["Accuracy"], cm_clinical_train08$overall["Accuracy"])
Table_cutoff <- data.frame(row.names=rownames, Sensitivity, Specificity, Accuracy)
# best trade-off between sensitvity and secificity: cutoff: 0.3

#Accuracy in test set

ED.test$yhat_clin03 <- as.factor(ifelse(ED.test$phat_clin > 0.3, 1, 0))
ED.test$admission <- as.factor(ED.test$admission)
cm_clinical_test <- confusionMatrix(ED.test$yhat_clin03, ED.test$admission, positive="1")
cm_clinical_test
```

```{r 6 Machine Learning Model, appendix  = TRUE, echo = FALSE, warning = FALSE, error = FALSE, message = FALSE}
#random forest
detach("package:MASS", unload = TRUE)
library(randomForest)

y_train <- ED.train$admission
x_train <- ED.train |> select(Sex, Age, NRS_pain, KTAS_RN, sbp, dbp, hr, resp, temp, arrival, injury, shock_index, mental_status) 

#tuning nodesize
set.seed(2404) 
nodesize <- seq(1, 50, 5)
acc <- sapply(nodesize, function(ns){
  train(data.frame(x_train), factor(y_train), method = "rf", 
               tuneGrid = data.frame(mtry = 5),
               nodesize = ns)$results$Accuracy
})
qplot(nodesize, acc)


#fit random forest model
set.seed(2333)
fit_rf <- randomForest(data.frame(x_train), factor(y_train), 
                       mtry = 5, nodesize = nodesize[which.max(acc)])

#random forest model performance in internal validation set
set.seed(2134)
ED.train$yhat_ML <- predict(fit_rf, type="response", newdata=ED.train)
ED.test$yhat_ML <- predict(fit_rf, type="response", newdata=ED.test)

#Accuracy in test set
ED.test$yhat_ML <- as.factor(ED.test$yhat_ML)
cm_ML_forest_test <- confusionMatrix(factor(ED.test$yhat_ML), factor(ED.test$admission), positive="1")
cm_ML_forest_test
```


# Conclusion
Summary of your question, methods and results
Additional topics can include:
Was your analysis successful? Why or why not?
What would you do if you had more time?


# References
Data source: https://www.kaggle.com/datasets/ilkeryildiz/emergency-service-triage-application
Original analysis: https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0216972


# Appendix
```{r ref.label=knitr::all_labels(appendix==TRUE), echo=FALSE, echo=TRUE}

```








